# Hydra Optuna Sweeper (TPE Sampler) for SpectraMind V50
# This configuration defines a robust, deterministic hyperparameter search using Optuna's TPE sampler.
# It is designed for physics-informed, neuro-symbolic training in the NeurIPS Ariel Data Challenge context.
# Reproducibility features:
# - Fixed sampler seed for determinism
# - Study naming with timestamp fallback handled by Hydra's job naming
# - Parameters mapped to canonical SpectraMind V50 config keys
# Usage:
# Train locally with Optuna TPE:
# python -m spectramind.cli.spectramind train -m hydra/sweeper=optuna_tpe
# Train with Submitit Slurm + Optuna TPE (multi-GPU/cluster):
# python -m spectramind.cli.spectramind train -m hydra/launcher=submitit_slurm hydra/sweeper=optuna_tpe
# Notes:
# - Direction is "minimize" for Generalized Log-Likelihood (GLL).
# - Override n_trials/n_jobs from CLI as needed: +hydra.sweeper.n_trials=200 +hydra.sweeper.n_jobs=2
# - Storage can be set to an SQLite/Postgres DSN for distributed sweeps: hydra.sweeper.storage="sqlite:///optuna.db"

defaults:
  - override /hydra/sweeper: optuna

hydra:
  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    direction: minimize
    study_name: spectramind_v50_optuna_tpe
    storage: null
    n_trials: 64
    n_jobs: 1
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
      multivariate: true
      group: true
    pruner:
      _target_: optuna.pruners.MedianPruner
      n_warmup_steps: 8
      n_startup_trials: 8
    params:
      optimizer.name: choice(adamw, lion)
      training.lr: tag(log, interval(1e-5, 5e-3))
      training.weight_decay: tag(log, interval(1e-6, 3e-3))
      scheduler.name: choice(cosine, one_cycle, plateau)
      scheduler.warmup_steps: choice(0, 250, 500, 1000)
      training.batch_size: choice(16, 32, 48, 64)
      training.grad_accum_steps: choice(1, 2, 4)
      model.encoder_dim: choice(256, 384, 512, 640, 768)
      model.dropout: interval(0.0, 0.45)
      model.decoder.hidden_dim: choice(256, 384, 512, 640)
      model.decoder.moe_experts: choice(1, 2, 4)
      loss.symbolic.weight: tag(log, interval(1e-4, 1e-1))
      loss.smoothness_l2.weight: tag(log, interval(1e-5, 1e-2))
      loss.fft_asymmetry.weight: tag(log, interval(1e-5, 1e-2))
      calib.temperature_init: interval(0.5, 3.0)
      calib.corel.weight: tag(log, interval(1e-4, 1e-1))
      model.uncertainty.dropout: interval(0.0, 0.3)

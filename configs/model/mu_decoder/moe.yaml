# Mixture-of-Experts μ decoder — SpectraMind V50
#
# Hydra path: model/mu_decoder/moe
#
# Compose via: defaults: [ model/mu_decoder@model.decoders.mu_decoder: moe ]

model:
  decoders:
    mu_decoder:
      enabled: true

      in_dim: 256
      out_bins: 283

      # Base trunk prior to experts (lightweight to keep router stable):
      hidden:
        dims: [384]
        activation: "silu"
        dropout: 0.05
        normalization: "layernorm"
        residual: false

      # MoE block config:
      moe:
        enabled: true
        n_experts: 4               # try 4–8 depending on memory/time budget
        top_k: 2                   # noisy-topk gating
        capacity_factor: 1.25      # token capacity multiplier per expert
        router: "noisy_topk"       # ["topk","noisy_topk"]
        aux_loss_weight: 0.01      # router load-balancing loss
        jitter_std: 0.0            # optional router noise

        # Expert MLPs:
        expert_hidden:
          dims: [512]              # per-expert hidden dim(s)
          activation: "silu"
          dropout: 0.10
          normalization: "layernorm"
          residual: false

      # Final projection after expert merge:
      head:
        type: "linear"
        final_activation: "none"
        init: "xavier_uniform"

      loss:
        type: "gll"
        weight: 1.0
        smooth:
          l2_weight: 0.010
          window: 5
        fft_smooth:
          weight: 0.000
          max_freq: null
        asymmetry:
          weight: 0.000
        nonneg:
          weight: 0.000

      multiscale:
        enabled: false

      export:
        save_intermediate: false
        save_dir: "artifacts/mu_decoder_moe"
      log:
        level: "INFO"
        include_shapes: true

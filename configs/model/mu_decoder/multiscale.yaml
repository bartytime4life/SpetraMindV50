# Multi-Scale μ decoder — SpectraMind V50
#
# Hydra path: model/mu_decoder/multiscale
#
# Compose via: defaults: [ model/mu_decoder@model.decoders.mu_decoder: multiscale ]

model:
  decoders:
    mu_decoder:
      enabled: true

      # Expect a pyramid of features from encoder fusion (e.g., [C, C/2, C/4] or similar).
      # If your encoder does not emit multiscale features, prefer base.yaml.
      in_dim: 256
      out_bins: 283

      multiscale:
        enabled: true
        # Number of scales the encoder exposes; your model code should adaptively accept a list of tensors.
        n_scales: 3
        # How to fuse scales before the MLP head:
        #  - "concat": channel-wise concat of pooled/upsampled scale features
        #  - "attn":   cross-scale attention fusion (heavier, sometimes better)
        fusion: "concat"           # ["concat","attn"]
        attn:
          heads: 4
          dropout: 0.10
          qkv_dim: 256            # internal attention projection size
        pooling:
          mode: "adaptive_avg"     # ["adaptive_avg","max","none"]
          target_bins: 283         # up/downsample each scale to this length before fusion

      hidden:
        dims: [512, 512]
        activation: "silu"
        dropout: 0.10
        normalization: "layernorm"
        residual: true

      head:
        type: "linear"
        final_activation: "none"
        init: "xavier_uniform"

      loss:
        type: "gll"
        weight: 1.0
        smooth:
          l2_weight: 0.012
          window: 5
        fft_smooth:
          weight: 0.000
          max_freq: null
        asymmetry:
          weight: 0.000
        nonneg:
          weight: 0.000

      moe:
        enabled: false

      export:
        save_intermediate: false
        save_dir: "artifacts/mu_decoder_multiscale"
      log:
        level: "INFO"
        include_shapes: true

# @package _global_.model.fusion
type: cross_attend
attn:
  layers: 2
  heads: 4
  qkv_bias: true
  dropout: 0.05
  resid_dropout: 0.05
  norm: layernorm
symbolic_injection:
  include_molecule_masks: false
  include_detector_seams: false
  include_wavelengths: false
  include_snr_weights: false
attention_bias_init: 0.0
export:
  attn_weights: true

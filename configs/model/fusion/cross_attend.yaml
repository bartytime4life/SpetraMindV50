# Cross-Attention Fusion Config â€” attends AIRS latents to FGS1 latents
fusion:
  enabled: true
  type: cross-attend
  dim: 256
  n_heads: 4
  dropout: 0.1
  layer_norm: true
  residual: true
  cross_attention_position: post-encoder  # where to insert cross-attention in the model
  attention_window: null                   # optional: restrict attention to wavelength windows
  export_attention: true                   # store attention maps for diagnostics/HTML dashboard
  export_step_latents: true                 # allow latent export for UMAP/t-SNE/FFT overlays
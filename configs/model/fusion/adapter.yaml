# @package _global_.model.fusion
type: adapter
adapter:
  bottleneck: 64
  activation: relu
  dropout: 0.05
  bias: true
mlp:
  hidden: [256]
  dropout: 0.05
